{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Text Example Experiment Runner Template\n",
    "11/6/2017 - Basic pipeline to run adversarial text generation experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "Base dataset: The Enron Spam Dataset: http://www2.aueb.gr/users/ion/data/enron-spam/ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Spam Preprocessing - UNIX Command line\n",
    "# 1. Removed all \\n and replaced with spaces: find . -type f -exec perl -i. -pe 's/\\r?\\n/ /' {} +\n",
    "# 2. Concatenated all spam into a single file and all ham into a single file.\n",
    "#      To concatenate within dirs: awk 1 enron1/ham/*.txt > enron1_ham.txt \n",
    "# 3. Randomly shuffled: shuf input > output\n",
    "# 4. Create 80, 10, 10 train, val, and test splits.\n",
    "\n",
    "# Total ham: 16545 messages; train/val/test = 13236, 1654, 1655\n",
    "# Total spam: 17171 messages; train/val/test = 13736, 1717, 1718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/spam.txt\n",
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/ham.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "classes = ['spam', 'ham']\n",
    "vocabulary_filename = 'email_train_vocab.txt' \n",
    "# Truncation and vocabulary shortening:\n",
    "# Using the train data only:\n",
    "# 1. Truncate both the spam and ham messages to truncation_len characters (adding padding where needed).\n",
    "# 2. From the truncated messages, compile a vocabulary of the class_vocabulary_size most frequent tokens for each class.\n",
    "# 3. Write a vocabulary file composed of the full, combined vocabulary (ie. the most frequent tokens across both classes.)\n",
    "truncation_len = 100\n",
    "class_vocabulary_size = 3000\n",
    "\n",
    "combined_vocab = []\n",
    "for class_name in classes:\n",
    "    filename = os.path.join(base_data_dir, 'train', class_name + '.txt') \n",
    "    print \"Now processing: %s\" % filename\n",
    "    with open(filename) as f:\n",
    "        all_lines = [line.strip().lower().split() for line in f.readlines()]\n",
    "        \n",
    "    # Truncate the files.\n",
    "    truncated_lines = [line[:truncation_len] for line in all_lines]\n",
    "    \n",
    "    # Add tokens to the counter\n",
    "    token_counts = Counter()\n",
    "    for line in truncated_lines:\n",
    "        token_counts.update(line)\n",
    "    combined_vocab += [elem for (elem, count) in token_counts.most_common(class_vocabulary_size)]\n",
    "    # Convert the combined vocabulary into a set.\n",
    "combined_vocab = set(combined_vocab)\n",
    "# Write out the combined_vocab to the vocabulary file\n",
    "with open(vocabulary_filename, 'w') as f:\n",
    "    for token in combined_vocab:\n",
    "        f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that takes in a file and a vocabulary file (which has a truncation len) and converts the text into\n",
    "# encoded/truncated sentences.\n",
    "\n",
    "class DatasetEncoderDecoder(object):\n",
    "    \"\"\"\n",
    "    Encodes and decodes sentences according to a vocabulary.\n",
    "    \n",
    "    Sentences are truncated. OOV words are assigned an <UNK> token, and <SOS>, <PAD>, and <EOS> tokens are added.\n",
    "    \n",
    "    truncation_len\n",
    "    \"\"\"\n",
    "    def __init__(self, truncation_len, vocab_file):\n",
    "        self.truncation_len = truncation_len\n",
    "        # Create index to word and word to index dicts from the vocab_file.\n",
    "        self.index2word = {0:'<SOS>', 1:'<EOS>', 2: '<UNK>', 3: '<PAD>'}\n",
    "        self.word2index = {'<SOS>':0, '<EOS>':1, '<UNK>':}\n",
    "        \n",
    "    \n",
    "    def encode_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Encodes a sentence according to the vocabulary.\n",
    "        Returns:\n",
    "            normalized: the normalized sentence, as it would be decoded.\n",
    "            encoded: the encoded numerical sentence.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def decode_sentence(self, encoded):\n",
    "        \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder - Seq2Seq Model\n",
    "Source: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

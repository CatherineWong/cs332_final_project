{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Text Example Experiment Runner Template\n",
    "11/6/2017 - Basic pipeline to run adversarial text generation experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "Base dataset: The Enron Spam Dataset: http://www2.aueb.gr/users/ion/data/enron-spam/ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "import sklearn.feature_extraction, sklearn.naive_bayes, sklearn.metrics, sklearn.externals\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Initialize some key paths\n",
    "base_dir = \"/cvgl2/u/catwong/cs332_final_project/\"\n",
    "base_data_dir = os.path.join(base_dir, 'data')\n",
    "base_checkpoints_dir = os.path.join(base_dir, 'checkpoints')\n",
    "\n",
    "nb_discriminator_ckpt = 'discriminator_multinomial_nb.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Spam Preprocessing - UNIX Command line\n",
    "# 1. Removed all \\n and replaced with spaces: find . -type f -exec perl -i. -pe 's/\\r?\\n/ /' {} +\n",
    "# 2. Concatenated all spam into a single file and all ham into a single file.\n",
    "#      To concatenate within dirs: awk 1 enron1/ham/*.txt > enron1_ham.txt \n",
    "# 3. Randomly shuffled: shuf input > output\n",
    "# 4. Create 80, 10, 10 train, val, and test splits.\n",
    "\n",
    "# Total ham: 16545 messages; train/val/test = 13236, 1654, 1655\n",
    "# Total spam: 17171 messages; train/val/test = 13736, 1717, 1718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/spam.txt\n",
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/ham.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "classes = ['spam', 'ham']\n",
    "vocabulary_filename = 'email_train_vocab.txt' \n",
    "# Truncation and vocabulary shortening:\n",
    "# Using the train data only:\n",
    "# 1. Truncate both the spam and ham messages to truncation_len characters (adding padding where needed).\n",
    "# 2. From the truncated messages, compile a vocabulary of the class_vocabulary_size most frequent tokens for each class.\n",
    "# 3. Write a vocabulary file composed of the full, combined vocabulary (ie. the most frequent tokens across both classes.)\n",
    "truncation_len = 100\n",
    "class_vocabulary_size = 3000\n",
    "\n",
    "combined_vocab = []\n",
    "for class_name in classes:\n",
    "    filename = os.path.join(base_data_dir, 'train', class_name + '.txt') \n",
    "    print \"Now processing: %s\" % filename\n",
    "    with open(filename) as f:\n",
    "        all_lines = [line.strip().lower().split() for line in f.readlines()]\n",
    "        \n",
    "    # Truncate the files.\n",
    "    truncated_lines = [line[:truncation_len] for line in all_lines]\n",
    "    \n",
    "    # Add tokens to the counter\n",
    "    token_counts = Counter()\n",
    "    for line in truncated_lines:\n",
    "        token_counts.update(line)\n",
    "    combined_vocab += [elem for (elem, count) in token_counts.most_common(class_vocabulary_size)]\n",
    "    # Convert the combined vocabulary into a set.\n",
    "combined_vocab = set(combined_vocab)\n",
    "# Write out the combined_vocab to the vocabulary file\n",
    "with open(vocabulary_filename, 'w') as f:\n",
    "    for token in combined_vocab:\n",
    "        f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: does your business depend on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple search engines . submit your website online and watch visitors stream to your e - business . best regards , myrtice melendez\n",
      "<SOS> subject: does your business <UNK> on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple <EOS>\n",
      "0 542 4302 2161 3459 2 2124 838 1689 2699 2129 2161 731 4305 1068 2161 731 781 2427 714 1398 1543 2161 1689 2227 3407 720 788 2256 1116 1136 1658 2001 3244 2161 731 302 2256 2973 183 1593 2161 731 1689 4060 772 4125 3203 2947 3062 302 3879 3960 997 3094 781 699 720 788 2256 217 925 2336 4252 4323 2161 731 1658 1246 2161 205 302 838 1756 1032 2336 585 1625 771 2336 793 2161 4140 137 781 4395 1789 925 2427 949 1397 302 4193 720 3351 720 1593 2161 731 781 1696 1\n",
      "<SOS> subject: does your business <UNK> on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Class that takes in a file and a vocabulary file (which has a truncation len) and converts the text into\n",
    "# encoded/truncated sentences.\n",
    "\n",
    "class DatasetEncoderDecoder(object):\n",
    "    \"\"\"\n",
    "    Encodes and decodes sentences according to a vocabulary.\n",
    "    \n",
    "    Sentences are truncated. OOV words are assigned an <UNK> token, and <SOS>, <PAD>, and <EOS> tokens are added.\n",
    "    \n",
    "    truncation_len\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file):\n",
    "        self.truncation_len = 100\n",
    "        # Create index to word and word to index dicts from the vocab_file.\n",
    "        num_default_tokens = 4\n",
    "        self.index2word = {0:'<SOS>', 1:'<EOS>', 2: '<UNK>', 3: '<PAD>'}\n",
    "        self.word2index = {'<SOS>':0, '<EOS>':1, '<UNK>': 2, '<PAD>': 3}\n",
    "        with open(vocab_file) as f:\n",
    "            all_lines = [line.strip() for line in f.readlines()]\n",
    "        for idx, token in enumerate(all_lines):\n",
    "            self.index2word[idx + num_default_tokens] = token\n",
    "            self.word2index[token] = idx + num_default_tokens\n",
    "          \n",
    "    def encode(self, sentence):\n",
    "        \"\"\"\n",
    "        Encodes a sentence according to the vocabulary.\n",
    "        Returns:\n",
    "            normalized: the normalized sentence, as it would be decoded.\n",
    "            encoded: the space-separated numerical sentence.\n",
    "        \"\"\"\n",
    "        truncated = sentence.lower().split()[:self.truncation_len]\n",
    "        truncated += ['<PAD>'] * max(self.truncation_len - len(truncated), 0)\n",
    "        truncated = ['<SOS>'] + truncated + ['<EOS>']\n",
    "        \n",
    "        normalized = []\n",
    "        encoded = []\n",
    "        # Encode, removing the UNK tokens\n",
    "        for token in truncated:\n",
    "            token = token if token in self.word2index else '<UNK>'\n",
    "            normalized.append(token)\n",
    "            encoded.append(str(self.word2index[token]))\n",
    "        \n",
    "        normalized = \" \".join(normalized)\n",
    "        encoded = \" \".join(encoded)\n",
    "        return normalized, encoded\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Returns the decoded sentence.\"\"\"\n",
    "        numerical_encoded = [int(token) for token in encoded.split()]\n",
    "        return \" \".join([self.index2word[token] for token in numerical_encoded])\n",
    "\n",
    "# Demonstration:\n",
    "vocab_file = 'data/email_train_vocab.txt'\n",
    "sample_text = 'Subject: does your business depend on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple search engines . submit your website online and watch visitors stream to your e - business . best regards , myrtice melendez'\n",
    "demo = DatasetEncoderDecoder(vocab_file)\n",
    "normalized, encoded = demo.encode(sample_text)\n",
    "print sample_text\n",
    "print normalized\n",
    "print encoded\n",
    "decoded = demo.decode(encoded)\n",
    "print decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the train, test, and text encoded files using this encoder.\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "splits = ['train', 'val', 'test']\n",
    "classes = ['spam.txt', 'ham.txt']\n",
    "truncation_len = 100\n",
    "vocab_file = 'data/email_train_vocab.txt'\n",
    "\n",
    "vocab_encoder = DatasetEncoderDecoder(vocab_file)\n",
    "for split in splits:\n",
    "    for class_file in classes:\n",
    "        raw_file = os.path.join(base_data_dir, split, class_file)\n",
    "        with open(raw_file) as f:\n",
    "            all_lines = [line.strip() for line in f.readlines()]\n",
    "        # Encode the lines\n",
    "        encoded_lines = [vocab_encoder.encode(line)[1] for line in all_lines]\n",
    "        \n",
    "        # Write out the encoded line\n",
    "        encoded_file = os.path.join(base_data_dir, split, 'encoded_' + class_file)\n",
    "        with open(encoded_file, 'w') as f:\n",
    "            for line in encoded_lines:\n",
    "                f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: /cvgl2/u/catwong/cs332_final_project/data/train/encoded_spam.txt\n",
      "Sample line: 0 542 672 3079 3946 2 4153 2 2780 2 3119 1755 1555 987 2402 771 2 2194 4305 2 3378 1613 672 2 1322 302 323 2881 1299 2421 1658 323 248 838 1299 4021 3946 2256 1352 3997 772 4403 4153 720 1852 2336 838 1613 672 2 2 4400 3658 2129 2 949 2780 2 3119 1755 1555 987 720 2 1399 2194 302 4427 720 3740 517 2129 2139 949 3119 1613 302 3081 302 3081 3534 2559 2707 2 2 2 2 302 2018 3267 3138 2607 302 3365 3473 2 501 2 2 302 4427 1\n",
      "Sample decoding: <SOS> subject: news alert ( <UNK> ) <UNK> orders <UNK> $ 3 million dollars what is <UNK> technologies ? <UNK> issued 2 news <UNK> today , one during market hours and one after the market closed ( you can view it below ) . according to the 2 news <UNK> <UNK> signed letters of <UNK> for orders <UNK> $ 3 million dollars . <UNK> max technologies , inc . announces letter of intent for $ 2 , 000 , 000 from a major <UNK> <UNK> <UNK> <UNK> , ny : march 29 , 2004 ; <UNK> - <UNK> <UNK> , inc <EOS>\n",
      "Sample file: /cvgl2/u/catwong/cs332_final_project/data/train/encoded_ham.txt\n",
      "Sample line: 0 542 2 2129 728 3797 2589 4193 2226 3936 2336 581 838 2 2129 838 728 3797 2589 720 838 3797 2589 2 3727 3459 143 1658 4370 2119 1658 4125 1149 2124 838 4040 3797 302 2992 302 1658 1469 3395 2 728 720 838 3797 2589 4125 2 838 1914 1469 2589 1658 4125 3406 838 1355 957 3267 1794 252 501 3134 1658 2621 302 728 976 720 495 2748 501 2 302 728 3578 2471 3431 1184 501 2 302 728 1608 577 4371 501 2975 2271 3537 3638 38 914 302 728 976 720 577 1\n",
      "Sample decoding: <SOS> subject: <UNK> of enron management committee i am pleased to announce the <UNK> of the enron management committee . the management committee <UNK> our business unit and function leadership and will focus on the key management , strategy , and policy issues <UNK> enron . the management committee will <UNK> the former policy committee and will include the following individuals : ken lay - chairman and ceo , enron corp . ray bowen - <UNK> , enron industrial markets michael brown - <UNK> , enron europe rick buy - exec vp & chief risk officer , enron corp . rick <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Samples of the encoded data\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "splits = ['train', 'val', 'test']\n",
    "classes = ['encoded_spam.txt', 'encoded_ham.txt']\n",
    "\n",
    "truncation_len = 100\n",
    "vocab_file = 'data/email_train_vocab.txt'\n",
    "vocab_encoder = DatasetEncoderDecoder(vocab_file)\n",
    "for class_file in classes:\n",
    "    sample_file = os.path.join(base_data_dir, splits[0], class_file)\n",
    "    print \"Sample file: \" + sample_file\n",
    "    with open(sample_file) as f:\n",
    "        all_lines = [line.strip() for line in f.readlines()]\n",
    "    sample_line = all_lines[0]\n",
    "    print \"Sample line: \" + sample_line\n",
    "    print \"Sample decoding: \" + vocab_encoder.decode(sample_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  542 1597 1132 3012 4078  660 2911 1453   29  838 2704 2109 1870 1658\n",
      " 4125 3928  838 3541  143 1011   36    2  224  638  954 2595  954  640  720\n",
      "  838 1355  771 3727 3544  971 1065  311  720  853 4096 3742 2559  720 2481\n",
      " 1332 4227 3720 4149  302 3404 1767 3645  302 3599 1012 3415  501 1703  501\n",
      " 2860   70 3415  501 1703  501  263 2349  501 1597 1132 3012 4078 1386  720\n",
      "  529    3    3    3    3    3    3    3    3    3    3    3    3    3    3\n",
      "    3    3    3    3    3    3    3    3    3    3    3    1]\n",
      "0\n",
      "<SOS> subject: calpine daily gas nomination we are still under the scheduled outage period and will bring the next unit down @ <UNK> saturday 03 / 24 / 01 . the following is our estimated burn until then . thanks > ricky a . archer fuel supply 700 louisiana , suite 2700 houston , texas 77002 713 - 830 - 8659 direct 713 - 830 - 8722 fax - calpine daily gas nomination 1 . doc <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <EOS>\n",
      "Total train examples: 26972, ham: 13236, spam: 13736\n",
      "Total val examples: 3371, ham: 1654, spam: 1717\n",
      "Total test examples: 3373, ham: 1655, spam: 1718\n"
     ]
    }
   ],
   "source": [
    "class SpamDataset(object):\n",
    "    \"\"\"\n",
    "    Dataset: encapsulates utility functions to get the dataset files.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_data_dir=\"/cvgl2/u/catwong/cs332_final_project/data/\",\n",
    "                 splits=['train', 'val', 'test'],\n",
    "                 label_names=['ham', 'spam'],\n",
    "                 encoded_files=['encoded_ham.txt', 'encoded_spam.txt'],\n",
    "                 vocab_file='/cvgl2/u/catwong/cs332_final_project/data/email_train_vocab.txt',\n",
    "                 random_seed=10):\n",
    "        self.base_data_dir = base_data_dir\n",
    "        self.splits = splits\n",
    "        self.label_names = label_names\n",
    "        self.encoded_files = encoded_files\n",
    "        self.vocab_encoder = DatasetEncoderDecoder(vocab_file)\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # Read in all of the lines from the files.\n",
    "        self.examples_dict = {}\n",
    "        self.labels_dict = {}\n",
    "        for split in splits:\n",
    "            all_examples = []\n",
    "            all_labels = []\n",
    "            for label, encoded_file in enumerate(encoded_files):\n",
    "                data_file = os.path.join(base_data_dir, split, encoded_file)\n",
    "                with open(data_file) as f:\n",
    "                    all_lines = [line.strip().split() for line in f.readlines()]\n",
    "                all_examples += all_lines\n",
    "                all_labels += [label] * len(all_lines)\n",
    "            self.examples_dict[split] = all_examples\n",
    "            self.labels_dict[split] = all_labels\n",
    "            \n",
    "    \n",
    "    def examples(self, \n",
    "                 split, \n",
    "                 shuffled=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: one of the splits (ex. train, val, test) with labels.\n",
    "            shuffled: whether to shuffle the examples.(default: True)\n",
    "        Returns:\n",
    "            examples: (list of lists)\n",
    "            labels: (list)\n",
    "        \"\"\"\n",
    "        examples = np.array(self.examples_dict[split]).astype(int)\n",
    "        labels = np.array(self.labels_dict[split])\n",
    "        if shuffled:\n",
    "            examples, labels = sklearn.utils.shuffle(examples, labels, random_state=self.random_seed)\n",
    "        return examples, labels\n",
    "    \n",
    "    def dataset_stats(self):\n",
    "        \"\"\"Prints useful stats about the dataset.\"\"\"\n",
    "        for split in self.splits:\n",
    "            labels = self.labels_dict[split]\n",
    "            num_pos = np.sum(labels)\n",
    "            num_neg = len(labels) - num_pos\n",
    "            print \"Total %s examples: %d, %s: %d, %s: %d\" % (split, len(labels), self.label_names[0], num_neg, self.label_names[1], num_pos)\n",
    "            \n",
    "\n",
    "# Demo\n",
    "dataset = SpamDataset()\n",
    "examples, labels =  dataset.examples(split='train', shuffled=True)\n",
    "print examples[0]\n",
    "print labels[0]\n",
    "print dataset.vocab_encoder.decode(\" \".join(examples[0].astype(str)))\n",
    "dataset.dataset_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "A general discriminator class and two implementations.\n",
    "\n",
    "NBDiscriminator (done, trained) and RNNDiscriminator (coming soon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "    Discriminator: a general discriminator class.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint=None):\n",
    "        pass\n",
    "    \n",
    "    def train(self, dataset):\n",
    "        raise Exception(\"Not implemented\")\n",
    "        \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        # Outputs a path that can be passed into the restore.\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def restore_model(self, model_checkpoint):\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "class MultinomialNBDiscriminator(Discriminator):\n",
    "    \"\"\"\n",
    "    MultinomialNB: Multinomial Naive Bayes Classifier w. alpha=1.0\n",
    "    \n",
    "    Trained using TF-IDF features.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint=None):\n",
    "        Discriminator.__init__(self, checkpoint)\n",
    "        if not checkpoint:\n",
    "            self.model = sklearn.naive_bayes.MultinomialNB()\n",
    "        else:\n",
    "            self.restore_model(checkpoint)\n",
    "    \n",
    "    def examples_to_term_doc(self, examples, num_terms=4480):\n",
    "        \"\"\"\n",
    "        Converts a numerically-encoded examples matrix into a sparse term-documents matrix.\n",
    "        \"\"\"\n",
    "        all_row_inds = all_col_inds = all_data = None\n",
    "        for row_ind, example in enumerate(examples):\n",
    "            if row_ind % 5000 == 0:\n",
    "                print \"Generating term-docs matrix: %d of %d\" %(row_ind, len(examples))\n",
    "            itemfreqs = scipy.stats.itemfreq(example).T\n",
    "            # Column indices: the term indices in that document.\n",
    "            col_inds = itemfreqs[0]\n",
    "            # Data: the counts of the terms in that document.\n",
    "            data = itemfreqs[1]\n",
    "            # Row indices: the current document, for each of the terms in that document.\n",
    "            row_inds = np.ones(itemfreqs.shape[1], dtype=np.int) * row_ind\n",
    "\n",
    "            # Concatenate to the existing data.\n",
    "            if all_row_inds is None:\n",
    "                all_row_inds = row_inds\n",
    "                all_col_inds = col_inds\n",
    "                all_data = data\n",
    "            else:\n",
    "                all_row_inds = np.append(all_row_inds, row_inds)\n",
    "                all_col_inds = np.append(all_col_inds, col_inds)\n",
    "                all_data = np.append(all_data, data)\n",
    "\n",
    "        num_docs = len(examples)\n",
    "        return scipy.sparse.csr_matrix((all_data, (all_row_inds, all_col_inds)), shape=(num_docs, num_terms))\n",
    "\n",
    "    def train(self, dataset):\n",
    "        examples, labels = dataset.examples(split='train', shuffled=True)\n",
    "        \n",
    "        # Silly way to compute sparse doc term matrix from examples matrix by converting it back into \"strings\".\n",
    "        self.train_counts = self.examples_to_term_doc(examples)\n",
    "        \n",
    "        # Featurize using TFIDF.\n",
    "        self.tf_transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "        X_transformed = self.tf_transformer.fit_transform(self.train_counts)\n",
    "        \n",
    "        # Fit the model to TFIDF counts.\n",
    "        self.model.fit(X_transformed, labels)\n",
    "    \n",
    "    def calculate_roc_auc(self, probs, labels):\n",
    "        # Probability estimates of the positive class.\n",
    "        pos_probs = probs[:, 1]\n",
    "        return sklearn.metrics.roc_auc_score(labels, pos_probs)\n",
    "    \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        # Get the test or validation examples.\n",
    "        examples, labels = dataset.examples(split=split, shuffled=True)\n",
    "        doc_terms = self.examples_to_term_doc(examples)\n",
    "        X_transformed = self.tf_transformer.transform(doc_terms)\n",
    "        \n",
    "        # Evaluate the model.\n",
    "        probs = self.model.predict_proba(doc_terms)\n",
    "        predicted = np.argmax(probs, axis=1)\n",
    "        \n",
    "        # Mean accuracy.\n",
    "        mean_accuracy = np.mean(predicted == labels)\n",
    "        print \"Mean_accuracy: %f\" % mean_accuracy\n",
    "        \n",
    "        # ROC-AUC Score.\n",
    "        roc_auc = self.calculate_roc_auc(probs, labels)\n",
    "        print \"ROC AUC: %f\" % roc_auc\n",
    "    \n",
    "    def save_model(self, \n",
    "                   checkpoint_dir='/cvgl2/u/catwong/cs332_final_project/checkpoints',\n",
    "                   checkpoint_name='multinomial_nb.pkl'):\n",
    "        sklearn.externals.joblib.dump(self.model, os.path.join(checkpoint_dir, checkpoint_name))\n",
    "    \n",
    "    def restore_model(self, model_checkpoint):\n",
    "        self.model = sklearn.externals.joblib.load(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "spam_dataset = SpamDataset()\n",
    "discriminator = MultinomialNBDiscriminator()\n",
    "discriminator.train(spam_dataset)\n",
    "discriminator.evaluate(spam_dataset, 'val')\n",
    "\n",
    "checkpoints_dir = '/cvgl2/u/catwong/cs332_final_project/checkpoints'\n",
    "checkpoint = discriminator.save_model()\n",
    "new_discriminator = MultinomialNBDiscriminator(checkpoint)\n",
    "discriminator.evaluate(spam_dataset, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Step 1: MultinomialNB Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating term-docs matrix: 0 of 26972\n",
      "Generating term-docs matrix: 5000 of 26972\n",
      "Generating term-docs matrix: 10000 of 26972\n",
      "Generating term-docs matrix: 15000 of 26972\n",
      "Generating term-docs matrix: 20000 of 26972\n",
      "Generating term-docs matrix: 25000 of 26972\n",
      "Generating term-docs matrix: 0 of 3371\n",
      "Mean_accuracy: 0.960249\n",
      "ROC AUC: 0.994239\n"
     ]
    }
   ],
   "source": [
    "# Train and save a model.\n",
    "spam_dataset = SpamDataset()\n",
    "discriminator = MultinomialNBDiscriminator()\n",
    "discriminator.train(spam_dataset)\n",
    "discriminator.evaluate(spam_dataset, 'val')\n",
    "\n",
    "checkpoint_dir = '/cvgl2/u/catwong/cs332_final_project/checkpoints'\n",
    "checkpoint_name = 'discriminator_multinomial_nb.pkl'\n",
    "checkpoint = discriminator.save_model(checkpoint_dir, checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "A general autoencoder class.\n",
    "Based on: https://github.com/MaximumEntropy/Seq2Seq-PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    \"\"\"\n",
    "    Autoencoder: a general discriminator class.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint=None):\n",
    "        pass\n",
    "    \n",
    "    def train(self, dataset):\n",
    "        raise Exception(\"Not implemented\")\n",
    "        \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        # Outputs a path that can be passed into the restore.\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def restore_model(self, model_checkpoint):\n",
    "        raise Exception(\"Not implemented\")\n",
    "        \n",
    "class Seq2SeqAutoencoder(Autoencoder):\n",
    "    def __init__(self, checkpoint=None):\n",
    "        Autoencoder.__init__(self, checkpoint)\n",
    "    \n",
    "    def train(self, dataset):\n",
    "        examples, labels = dataset.examples(split='train', shuffled=True)\n",
    "        \n",
    "        # Convert the input examples into paired tensors.\n",
    "        \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        # Outputs a path that can be passed into the restore.\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def restore_model(self, model_checkpoint):\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "# Demo\n",
    "spam_dataset = SpamDataset()\n",
    "autoencoder = Seq2SeqAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

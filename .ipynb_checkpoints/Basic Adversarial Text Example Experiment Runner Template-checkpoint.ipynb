{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Text Example Experiment Runner Template\n",
    "11/6/2017 - Basic pipeline to run adversarial text generation experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "Base dataset: The Enron Spam Dataset: http://www2.aueb.gr/users/ion/data/enron-spam/ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n",
      "Use CUDA:True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "import sklearn.feature_extraction, sklearn.naive_bayes, sklearn.metrics, sklearn.externals\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from seq2seq.model import Seq2Seq, Seq2SeqAutoencoder\n",
    "\n",
    "# Set CUDA Visible devices\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print \"Use CUDA:\" + str(use_cuda)\n",
    "\n",
    "# Initialize some key paths\n",
    "base_dir = \"/cvgl2/u/catwong/cs332_final_project/\"\n",
    "base_data_dir = os.path.join(base_dir, 'data')\n",
    "base_checkpoints_dir = os.path.join(base_dir, 'checkpoints')\n",
    "\n",
    "nb_discriminator_ckpt = 'discriminator_multinomial_nb.pkl'\n",
    "\n",
    "# Other constants.\n",
    "# The number of terms, including special tokens, in the final vocabulary.\n",
    "TRAINING_VOCAB_SIZE = {\n",
    "    100: 4480,\n",
    "    30: 4628\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spam Preprocessing - UNIX Command line\n",
    "# 1. Removed all \\n and replaced with spaces: find . -type f -exec perl -i. -pe 's/\\r?\\n/ /' {} +\n",
    "# 2. Concatenated all spam into a single file and all ham into a single file.\n",
    "#      To concatenate within dirs: awk 1 enron1/ham/*.txt > enron1_ham.txt \n",
    "# 3. Randomly shuffled: shuf input > output\n",
    "# 4. Create 80, 10, 10 train, val, and test splits.\n",
    "\n",
    "# Total ham: 16545 messages; train/val/test = 13236, 1654, 1655\n",
    "# Total spam: 17171 messages; train/val/test = 13736, 1717, 1718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/spam.txt\n",
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/ham.txt\n",
      "4624\n",
      "30_email_train_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "classes = ['spam', 'ham']\n",
    "vocabulary_filename = '30_email_train_vocab.txt' \n",
    "# Truncation and vocabulary shortening:\n",
    "# Using the train data only:\n",
    "# 1. Truncate both the spam and ham messages to truncation_len characters (adding padding where needed).\n",
    "# 2. From the truncated messages, compile a vocabulary of the class_vocabulary_size most frequent tokens for each class.\n",
    "# 3. Write a vocabulary file composed of the full, combined vocabulary (ie. the most frequent tokens across both classes.)\n",
    "truncation_len = 30\n",
    "class_vocabulary_size = 3000\n",
    "\n",
    "combined_vocab = []\n",
    "for class_name in classes:\n",
    "    filename = os.path.join(base_data_dir, 'train', class_name + '.txt') \n",
    "    print \"Now processing: %s\" % filename\n",
    "    with open(filename) as f:\n",
    "        all_lines = [line.strip().lower().split() for line in f.readlines()]\n",
    "        \n",
    "    # Truncate the files.\n",
    "    truncated_lines = [line[:truncation_len] for line in all_lines]\n",
    "    \n",
    "    # Add tokens to the counter\n",
    "    token_counts = Counter()\n",
    "    for line in truncated_lines:\n",
    "        token_counts.update(line)\n",
    "    combined_vocab += [elem for (elem, count) in token_counts.most_common(class_vocabulary_size)]\n",
    "\n",
    "# Convert the combined vocabulary into a set.\n",
    "combined_vocab = set(combined_vocab)\n",
    "print len(combined_vocab)\n",
    "# Write out the combined_vocab to the vocabulary file\n",
    "print vocabulary_filename\n",
    "with open(os.path.join(base_data_dir, vocabulary_filename), 'w') as f:\n",
    "    for token in combined_vocab:\n",
    "        f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: does your business depend on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple search engines . submit your website online and watch visitors stream to your e - business . best regards , myrtice melendez\n",
      "<SOS> subject: does your business <UNK> on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple <EOS>\n",
      "0 542 4302 2161 3459 2 2124 838 1689 2699 2129 2161 731 4305 1068 2161 731 781 2427 714 1398 1543 2161 1689 2227 3407 720 788 2256 1116 1136 1658 2001 3244 2161 731 302 2256 2973 183 1593 2161 731 1689 4060 772 4125 3203 2947 3062 302 3879 3960 997 3094 781 699 720 788 2256 217 925 2336 4252 4323 2161 731 1658 1246 2161 205 302 838 1756 1032 2336 585 1625 771 2336 793 2161 4140 137 781 4395 1789 925 2427 949 1397 302 4193 720 3351 720 1593 2161 731 781 1696 1\n",
      "<SOS> subject: does your business <UNK> on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Class that takes in a file and a vocabulary file (which has a truncation len) and converts the text into\n",
    "# encoded/truncated sentences.\n",
    "\n",
    "class DatasetEncoderDecoder(object):\n",
    "    \"\"\"\n",
    "    Encodes and decodes sentences according to a vocabulary.\n",
    "    \n",
    "    Sentences are truncated. OOV words are assigned an <UNK> token, and <SOS>, <PAD>, and <EOS> tokens are added.\n",
    "    \n",
    "    truncation_len\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file, truncation_len=100):\n",
    "        self.truncation_len = truncation_len\n",
    "        # Create index to word and word to index dicts from the vocab_file.\n",
    "        num_default_tokens = 4\n",
    "        self.index2word = {0:'<SOS>', 1:'<EOS>', 2: '<UNK>', 3: '<PAD>'}\n",
    "        self.word2index = {'<SOS>':0, '<EOS>':1, '<UNK>': 2, '<PAD>': 3}\n",
    "        with open(vocab_file) as f:\n",
    "            all_lines = [line.strip() for line in f.readlines()]\n",
    "        for idx, token in enumerate(all_lines):\n",
    "            self.index2word[idx + num_default_tokens] = token\n",
    "            self.word2index[token] = idx + num_default_tokens\n",
    "          \n",
    "    def encode(self, sentence):\n",
    "        \"\"\"\n",
    "        Encodes a sentence according to the vocabulary.\n",
    "        Returns:\n",
    "            normalized: the normalized sentence, as it would be decoded.\n",
    "            encoded: the space-separated numerical sentence.\n",
    "        \"\"\"\n",
    "        truncated = sentence.lower().split()[:self.truncation_len]\n",
    "        truncated += ['<PAD>'] * max(self.truncation_len - len(truncated), 0)\n",
    "        truncated = ['<SOS>'] + truncated + ['<EOS>']\n",
    "        \n",
    "        normalized = []\n",
    "        encoded = []\n",
    "        # Encode, removing the UNK tokens\n",
    "        for token in truncated:\n",
    "            token = token if token in self.word2index else '<UNK>'\n",
    "            normalized.append(token)\n",
    "            encoded.append(str(self.word2index[token]))\n",
    "        \n",
    "        normalized = \" \".join(normalized)\n",
    "        encoded = \" \".join(encoded)\n",
    "        return normalized, encoded\n",
    "    \n",
    "    def decode_numpy(self, numerical_encoded):\n",
    "        \"\"\"Returns the decoded sentence.\"\"\"\n",
    "        return \" \".join([self.index2word[token] for token in numerical_encoded])\n",
    "\n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Returns the decoded sentence.\"\"\"\n",
    "        numerical_encoded = [int(token) for token in encoded.split()]\n",
    "        return \" \".join([self.index2word[token] for token in numerical_encoded])\n",
    "\n",
    "# Demonstration:\n",
    "vocab_file = 'data/email_train_vocab.txt'\n",
    "sample_text = 'Subject: does your business depend on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple search engines . submit your website online and watch visitors stream to your e - business . best regards , myrtice melendez'\n",
    "demo = DatasetEncoderDecoder(vocab_file)\n",
    "normalized, encoded = demo.encode(sample_text)\n",
    "print sample_text\n",
    "print normalized\n",
    "print encoded\n",
    "decoded = demo.decode(encoded)\n",
    "print decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the train, test, and text encoded files using this encoder.\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "splits = ['train', 'val', 'test']\n",
    "classes = ['spam.txt', 'ham.txt']\n",
    "truncation_len = 100\n",
    "vocab_file = 'data/100_email_train_vocab.txt'\n",
    "\n",
    "vocab_encoder = DatasetEncoderDecoder(vocab_file, truncation_len=truncation_len)\n",
    "for split in splits:\n",
    "    for class_file in classes:\n",
    "        raw_file = os.path.join(base_data_dir, split, class_file)\n",
    "        with open(raw_file) as f:\n",
    "            all_lines = [line.strip() for line in f.readlines()]\n",
    "        # Encode the lines\n",
    "        encoded_lines = [vocab_encoder.encode(line)[1] for line in all_lines]\n",
    "        \n",
    "        # Write out the encoded line\n",
    "        encoded_file = os.path.join(base_data_dir, split, str(truncation_len) + '_encoded_' + class_file)\n",
    "        with open(encoded_file, 'w') as f:\n",
    "            for line in encoded_lines:\n",
    "                f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: /cvgl2/u/catwong/cs332_final_project/data/train/30_encoded_spam.txt\n",
      "Sample line: 0 561 688 3176 4083 2 4287 2 2861 2 3221 1793 1586 997 2445 788 2 2236 4440 2 3503 1649 688 2 1351 319 342 2969 1330 2467 1694 1\n",
      "Sample decoding: <SOS> subject: news alert ( <UNK> ) <UNK> orders <UNK> $ 3 million dollars what is <UNK> technologies ? <UNK> issued 2 news <UNK> today , one during market hours and <EOS>\n",
      "Sample file: /cvgl2/u/catwong/cs332_final_project/data/train/30_encoded_ham.txt\n",
      "Sample line: 0 561 4511 2173 3084 3942 1830 4327 2265 4075 2378 596 853 4511 2173 853 3084 3942 1830 740 853 3942 1830 2 3866 3591 970 1694 4514 2159 1694 1\n",
      "Sample decoding: <SOS> subject: formation of enron management committee i am pleased to announce the formation of the enron management committee . the management committee <UNK> our business unit and function leadership and <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Samples of the encoded data\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "splits = ['train', 'val', 'test']\n",
    "classes = ['encoded_spam.txt', 'encoded_ham.txt']\n",
    "\n",
    "truncation_len = 30\n",
    "vocab_file = 'data/30_email_train_vocab.txt'\n",
    "vocab_encoder = DatasetEncoderDecoder(vocab_file, truncation_len=truncation_len)\n",
    "for class_file in classes:\n",
    "    sample_file = os.path.join(base_data_dir, splits[0], str(truncation_len) + \"_\" + class_file)\n",
    "    print \"Sample file: \" + sample_file\n",
    "    with open(sample_file) as f:\n",
    "        all_lines = [line.strip() for line in f.readlines()]\n",
    "    sample_line = all_lines[0]\n",
    "    print \"Sample line: \" + sample_line\n",
    "    print \"Sample decoding: \" + vocab_encoder.decode(sample_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  542    2 2129  728 3797 2589 4193 2226 3936 2336  581  838    2 2129\n",
      "  838  728 3797 2589  720  838 3797 2589    2 3727 3459  143 1658 4370 2119\n",
      " 1658 4125 1149 2124  838 4040 3797  302 2992  302 1658 1469 3395    2  728\n",
      "  720  838 3797 2589 4125    2  838 1914 1469 2589 1658 4125 3406  838 1355\n",
      "  957 3267 1794  252  501 3134 1658 2621  302  728  976  720  495 2748  501\n",
      "    2  302  728 3578 2471 3431 1184  501    2  302  728 1608  577 4371  501\n",
      " 2975 2271 3537 3638   38  914  302  728  976  720  577    1]\n",
      "0\n",
      "<SOS> subject: <UNK> of enron management committee i am pleased to announce the <UNK> of the enron management committee . the management committee <UNK> our business unit and function leadership and will focus on the key management , strategy , and policy issues <UNK> enron . the management committee will <UNK> the former policy committee and will include the following individuals : ken lay - chairman and ceo , enron corp . ray bowen - <UNK> , enron industrial markets michael brown - <UNK> , enron europe rick buy - exec vp & chief risk officer , enron corp . rick <EOS>\n",
      "Total train examples: 26972, ham: 13236, spam: 13736\n",
      "Total val examples: 3371, ham: 1654, spam: 1717\n",
      "Total test examples: 3373, ham: 1655, spam: 1718\n"
     ]
    }
   ],
   "source": [
    "class SpamDataset(object):\n",
    "    \"\"\"\n",
    "    Dataset: encapsulates utility functions to get the dataset files.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_data_dir=\"/cvgl2/u/catwong/cs332_final_project/data/\",\n",
    "                 splits=['train', 'val', 'test'],\n",
    "                 label_names=['ham', 'spam'],\n",
    "                 truncation_len=100,\n",
    "                 encoded_files=['encoded_ham.txt', 'encoded_spam.txt'],\n",
    "                 vocab_file='email_train_vocab.txt',\n",
    "                 random_seed=10):\n",
    "        self.base_data_dir = base_data_dir\n",
    "        self.splits = splits\n",
    "        self.label_names = label_names\n",
    "        self.encoded_files = [str(truncation_len) + \"_\" + f for f in encoded_files]\n",
    "        self.vocab_file = os.path.join(base_data_dir, str(truncation_len) + \"_\" + vocab_file)\n",
    "        self.vocab_encoder = DatasetEncoderDecoder(self.vocab_file, truncation_len=truncation_len)\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # Read in all of the lines from the files.\n",
    "        self.examples_dict = {}\n",
    "        self.labels_dict = {}\n",
    "        for split in splits:\n",
    "            all_examples = []\n",
    "            all_labels = []\n",
    "            for label, encoded_file in enumerate(self.encoded_files):\n",
    "                data_file = os.path.join(base_data_dir, split, encoded_file)\n",
    "                with open(data_file) as f:\n",
    "                    all_lines = [line.strip().split() for line in f.readlines()]\n",
    "                all_examples += all_lines\n",
    "                all_labels += [label] * len(all_lines)\n",
    "            self.examples_dict[split] = all_examples\n",
    "            self.labels_dict[split] = all_labels\n",
    "            \n",
    "    \n",
    "    def examples(self, \n",
    "                 split, \n",
    "                 shuffled=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: one of the splits (ex. train, val, test) with labels.\n",
    "            shuffled: whether to shuffle the examples.(default: True)\n",
    "        Returns:\n",
    "            examples: (list of lists)\n",
    "            labels: (list)\n",
    "        \"\"\"\n",
    "        examples = np.array(self.examples_dict[split]).astype(int)\n",
    "        labels = np.array(self.labels_dict[split])\n",
    "        if shuffled:\n",
    "            examples, labels = sklearn.utils.shuffle(examples, labels, random_state=self.random_seed)\n",
    "        return examples, labels\n",
    "    \n",
    "    def dataset_stats(self):\n",
    "        \"\"\"Prints useful stats about the dataset.\"\"\"\n",
    "        for split in self.splits:\n",
    "            labels = self.labels_dict[split]\n",
    "            num_pos = np.sum(labels)\n",
    "            num_neg = len(labels) - num_pos\n",
    "            print \"Total %s examples: %d, %s: %d, %s: %d\" % (split, len(labels), self.label_names[0], num_neg, self.label_names[1], num_pos)\n",
    "            \n",
    "\n",
    "# Demo\n",
    "dataset = SpamDataset(truncation_len=100)\n",
    "examples, labels =  dataset.examples(split='train', shuffled=False)\n",
    "print examples[0]\n",
    "print labels[0]\n",
    "print dataset.vocab_encoder.decode(\" \".join(examples[0].astype(str)))\n",
    "dataset.dataset_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "A general discriminator class and two implementations.\n",
    "\n",
    "NBDiscriminator (done, trained) and RNNDiscriminator (coming soon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "    Discriminator: a general discriminator class.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint=None):\n",
    "        pass\n",
    "    \n",
    "    def train(self, dataset):\n",
    "        raise Exception(\"Not implemented\")\n",
    "        \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        # Outputs a path that can be passed into the restore.\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def restore_model(self, model_checkpoint):\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "class MultinomialNBDiscriminator(Discriminator):\n",
    "    \"\"\"\n",
    "    MultinomialNB: Multinomial Naive Bayes Classifier w. alpha=1.0\n",
    "    \n",
    "    Trained using TF-IDF features.\n",
    "    \"\"\"\n",
    "    def __init__(self, truncation_len=100, checkpoint=None):\n",
    "        Discriminator.__init__(self, checkpoint)\n",
    "        self.truncation_len=truncation_len\n",
    "        if not checkpoint:\n",
    "            self.model = sklearn.naive_bayes.MultinomialNB()\n",
    "        else:\n",
    "            self.restore_model(checkpoint)\n",
    "    \n",
    "    def examples_to_term_doc(self, examples):\n",
    "        \"\"\"\n",
    "        Converts a numerically-encoded examples matrix into a sparse term-documents matrix.\n",
    "        \"\"\"\n",
    "        num_terms = TRAINING_VOCAB_SIZE[self.truncation_len]\n",
    "        all_row_inds = all_col_inds = all_data = None\n",
    "        for row_ind, example in enumerate(examples):\n",
    "            if row_ind % 5000 == 0:\n",
    "                print \"Generating term-docs matrix: %d of %d\" %(row_ind, len(examples))\n",
    "            itemfreqs = scipy.stats.itemfreq(example).T\n",
    "            # Column indices: the term indices in that document.\n",
    "            col_inds = itemfreqs[0]\n",
    "            # Data: the counts of the terms in that document.\n",
    "            data = itemfreqs[1]\n",
    "            # Row indices: the current document, for each of the terms in that document.\n",
    "            row_inds = np.ones(itemfreqs.shape[1], dtype=np.int) * row_ind\n",
    "\n",
    "            # Concatenate to the existing data.\n",
    "            if all_row_inds is None:\n",
    "                all_row_inds = row_inds\n",
    "                all_col_inds = col_inds\n",
    "                all_data = data\n",
    "            else:\n",
    "                all_row_inds = np.append(all_row_inds, row_inds)\n",
    "                all_col_inds = np.append(all_col_inds, col_inds)\n",
    "                all_data = np.append(all_data, data)\n",
    "\n",
    "        num_docs = len(examples)\n",
    "        return scipy.sparse.csr_matrix((all_data, (all_row_inds, all_col_inds)), shape=(num_docs, num_terms))\n",
    "\n",
    "    def train(self, dataset):\n",
    "        examples, labels = dataset.examples(split='train', shuffled=True)\n",
    "        \n",
    "        # Silly way to compute sparse doc term matrix from examples matrix by converting it back into \"strings\".\n",
    "        self.train_counts = self.examples_to_term_doc(examples)\n",
    "        \n",
    "        # Featurize using TFIDF.\n",
    "        self.tf_transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "        X_transformed = self.tf_transformer.fit_transform(self.train_counts)\n",
    "        \n",
    "        # Fit the model to TFIDF counts.\n",
    "        self.model.fit(X_transformed, labels)\n",
    "    \n",
    "    def calculate_roc_auc(self, probs, labels):\n",
    "        # Probability estimates of the positive class.\n",
    "        pos_probs = probs[:, 1]\n",
    "        return sklearn.metrics.roc_auc_score(labels, pos_probs)\n",
    "    \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        # Get the test or validation examples.\n",
    "        examples, labels = dataset.examples(split=split, shuffled=True)\n",
    "        doc_terms = self.examples_to_term_doc(examples)\n",
    "        X_transformed = self.tf_transformer.transform(doc_terms)\n",
    "        \n",
    "        # Evaluate the model.\n",
    "        probs = self.model.predict_proba(doc_terms)\n",
    "        predicted = np.argmax(probs, axis=1)\n",
    "        \n",
    "        # Mean accuracy.\n",
    "        mean_accuracy = np.mean(predicted == labels)\n",
    "        print \"Mean_accuracy: %f\" % mean_accuracy\n",
    "        \n",
    "        # ROC-AUC Score.\n",
    "        roc_auc = self.calculate_roc_auc(probs, labels)\n",
    "        print \"ROC AUC: %f\" % roc_auc\n",
    "    \n",
    "    def save_model(self, \n",
    "                   checkpoint_dir='/cvgl2/u/catwong/cs332_final_project/checkpoints',\n",
    "                   checkpoint_name='multinomial_nb'):\n",
    "        # Separately pickles the model and the transformer.\n",
    "        checkpoint = os.path.join(checkpoint_dir, str(self.truncation_len) + \"_\" + checkpoint_name)\n",
    "        sklearn.externals.joblib.dump(self.model, checkpoint + \"_model.pkl\")\n",
    "        sklearn.externals.joblib.dump(self.tf_transformer, checkpoint + \"_tf_transformer.pkl\")\n",
    "        return [checkpoint + \"_model.pkl\", checkpoint + \"_tf_transformer.pkl\"]\n",
    "    \n",
    "    def restore_model(self, model_checkpoints):\n",
    "        self.model = sklearn.externals.joblib.load(model_checkpoints[0])\n",
    "        self.tf_transformer = sklearn.externals.joblib.load(model_checkpoints[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating term-docs matrix: 0 of 3371\n",
      "Generating term-docs matrix: 0 of 3371\n",
      "Mean_accuracy: 0.970039\n",
      "ROC AUC: 0.993406\n",
      "['/cvgl2/u/catwong/cs332_final_project/checkpoints/30_multinomial_nb_model.pkl', '/cvgl2/u/catwong/cs332_final_project/checkpoints/30_multinomial_nb_tf_transformer.pkl']\n",
      "Generating term-docs matrix: 0 of 3371\n",
      "Mean_accuracy: 0.970039\n",
      "ROC AUC: 0.993406\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "spam_dataset = SpamDataset(truncation_len=30)\n",
    "discriminator = MultinomialNBDiscriminator(truncation_len=30)\n",
    "discriminator.train(spam_dataset)\n",
    "discriminator.evaluate(spam_dataset, 'val')\n",
    "\n",
    "checkpoints_dir = '/cvgl2/u/catwong/cs332_final_project/checkpoints'\n",
    "checkpoint = discriminator.save_model()\n",
    "print checkpoint\n",
    "new_discriminator = MultinomialNBDiscriminator(checkpoint=checkpoint, truncation_len=30)\n",
    "new_discriminator.evaluate(spam_dataset, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Step 1: MultinomialNB Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on truncation_len: 30\n",
      "Generating term-docs matrix: 0 of 26972\n",
      "Generating term-docs matrix: 5000 of 26972\n",
      "Generating term-docs matrix: 10000 of 26972\n",
      "Generating term-docs matrix: 15000 of 26972\n",
      "Generating term-docs matrix: 20000 of 26972\n",
      "Generating term-docs matrix: 25000 of 26972\n",
      "Generating term-docs matrix: 0 of 3371\n",
      "Mean_accuracy: 0.973598\n",
      "ROC AUC: 0.994559\n",
      "Generating term-docs matrix: 0 of 3371\n",
      "Mean_accuracy: 0.973598\n",
      "ROC AUC: 0.994559\n",
      "Now on truncation_len: 100\n",
      "Generating term-docs matrix: 0 of 26972\n",
      "Generating term-docs matrix: 5000 of 26972\n",
      "Generating term-docs matrix: 10000 of 26972\n",
      "Generating term-docs matrix: 15000 of 26972\n",
      "Generating term-docs matrix: 20000 of 26972\n",
      "Generating term-docs matrix: 25000 of 26972\n",
      "Generating term-docs matrix: 0 of 3371\n",
      "Mean_accuracy: 0.960249\n",
      "ROC AUC: 0.994239\n",
      "Generating term-docs matrix: 0 of 3371\n",
      "Mean_accuracy: 0.960249\n",
      "ROC AUC: 0.994239\n"
     ]
    }
   ],
   "source": [
    "# Train and save a model.\n",
    "for truncation_len in [30, 100]:\n",
    "    print \"Now on truncation_len: \" + str(truncation_len)\n",
    "    spam_dataset = SpamDataset(truncation_len=truncation_len)\n",
    "    discriminator = MultinomialNBDiscriminator(truncation_len=truncation_len)\n",
    "    discriminator.train(spam_dataset)\n",
    "    discriminator.evaluate(spam_dataset, 'val')\n",
    "\n",
    "    checkpoints_dir = '/cvgl2/u/catwong/cs332_final_project/checkpoints'\n",
    "    checkpoint = discriminator.save_model()\n",
    "    new_discriminator = MultinomialNBDiscriminator(checkpoint=checkpoint, truncation_len=truncation_len)\n",
    "    new_discriminator.evaluate(spam_dataset, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "A general autoencoder class.\n",
    "Based on: https://github.com/MaximumEntropy/Seq2Seq-PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Minibatch : 0 Loss : 8.40566\n",
      "pa vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent vincent hotat hotat corner found hotat hotat hotat canadian canadian canadian canadian canadian canadian sequoia canadian canadian canadian canadian canadian sequoia canadian canadian canadian vincent hotat canadian canadian canadian 96 sequoia 96 hotat hotat sequoia 96 sequoia hotat hotat hotat canadian sequoia hotat found found hotat hotat hotat hotat edge else edge canadian canadian hotat sequoia corner hotat hotat hotat hotat sequoia sequoia hotat hotat sequoia hotat acy hotat canadian canadian canadian hotat hotat hotat canadian canadian\n",
      "<SOS> subject: calpine daily gas nomination we are still under the scheduled outage period and will bring the next unit down @ <UNK> saturday 03 / 24 / 01 . the following is our estimated burn until then . thanks > ricky a . archer fuel supply 700 louisiana , suite 2700 houston , texas 77002 713 - 830 - 8659 direct 713 - 830 - 8722 fax - calpine daily gas nomination 1 . doc <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <EOS>\n",
      "pa pa corner sequoia canadian sequoia hotat hotat hotat hotat corner hotat sequoia hotat sequoia sequoia vincent hotat 96 96 hotat edge edge laser hotat hotat hotat hotat march canadian sequoia edge hotat hotat hotat sequoia seeing canadian canadian hotat hotat edge else hotat hotat hotat hotat found hotat found sequoia sequoia hotat hotat hotat hotat edge canadian edge hotat hotat hotat hotat hotat hotat edge canadian hotat canadian canadian hotat submit hotat hotat hotat hotat sequoia hotat sequoia hotat 96 96 96 hotat hotat hotat hotat hotat edge edge canadian canadian canadian canadian canadian canadian canadian edge hotat canadian canadian canadian\n",
      "<SOS> subject: windows 2000 / outlook project houston - area et & s employees : ( the marketing group should have already been contacted by jean <UNK> . ) next week , we are going to begin phase i of the windows 2000 / outlook project . in this phase , we will be gathering information about the hardware and software that you are using within your group . the process will begin with an email <UNK> . you will receive an email on monday , september 25 , with the subject of w 2 k <UNK> - please open . <EOS>\n",
      "Epoch : 1 Minibatch : 0 Loss : 8.39966\n",
      "pa vincent vincent vincent enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed enjoyed hotat hotat corner found hotat hotat hotat canadian canadian canadian canadian canadian canadian sequoia canadian canadian canadian canadian canadian sequoia canadian canadian canadian vincent hotat canadian canadian canadian 96 sequoia hotat hotat hotat sequoia 96 sequoia hotat hotat hotat hotat sequoia hotat found found hotat hotat hotat hotat edge else edge canadian canadian hotat sequoia corner hotat hotat hotat hotat sequoia sequoia hotat hotat sequoia hotat hotat hotat canadian canadian canadian hotat hotat hotat canadian canadian\n",
      "<SOS> subject: calpine daily gas nomination we are still under the scheduled outage period and will bring the next unit down @ <UNK> saturday 03 / 24 / 01 . the following is our estimated burn until then . thanks > ricky a . archer fuel supply 700 louisiana , suite 2700 houston , texas 77002 713 - 830 - 8659 direct 713 - 830 - 8722 fax - calpine daily gas nomination 1 . doc <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <EOS>\n",
      "pa pa corner sequoia canadian sequoia hotat hotat hotat hotat corner hotat hotat hotat sequoia sequoia vincent hotat 96 96 march edge edge laser laser hotat hotat hotat march canadian sequoia edge hotat hotat hotat sequoia seeing canadian canadian hotat calpine edge else hotat hotat hotat hotat found hotat found sequoia sequoia hotat hotat hotat hotat edge calpine edge hotat hotat hotat hotat hotat hotat edge canadian hotat canadian canadian hotat laser hotat hotat hotat hotat edge hotat sequoia hotat 96 96 96 hotat hotat hotat hotat hotat edge edge canadian canadian calpine canadian canadian canadian calpine edge hotat hotat canadian canadian\n",
      "<SOS> subject: windows 2000 / outlook project houston - area et & s employees : ( the marketing group should have already been contacted by jean <UNK> . ) next week , we are going to begin phase i of the windows 2000 / outlook project . in this phase , we will be gathering information about the hardware and software that you are using within your group . the process will begin with an email <UNK> . you will receive an email on monday , september 25 , with the subject of w 2 k <UNK> - please open . <EOS>\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder(object):\n",
    "    \"\"\"\n",
    "    Autoencoder: a general discriminator class.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint=None, dataset=None):\n",
    "        pass\n",
    "    \n",
    "    def train(self, dataset):\n",
    "        raise Exception(\"Not implemented\")\n",
    "        \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        # Outputs a path that can be passed into the restore.\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def restore_model(self, model_checkpoint):\n",
    "        raise Exception(\"Not implemented\")\n",
    "        \n",
    "class SpamSeq2SeqAutoencoder(Autoencoder):\n",
    "    \"\"\"\n",
    "    SpamSeq2Seq Autoencoder.\n",
    "    Implementation from: https://github.com/MaximumEntropy/Seq2Seq-PyTorch\n",
    "    Uses the following config: config_en_autoencoder_1_billion.json\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint=None, dataset=None):\n",
    "        Autoencoder.__init__(self, checkpoint, dataset)\n",
    "        self.dataset = dataset \n",
    "        self.vocab_size = len(self.dataset.vocab_encoder.word2index)\n",
    "        self.pad_token_ind = self.dataset.vocab_encoder.word2index['<PAD>']\n",
    "        self.batch_size = 2\n",
    "        \n",
    "        # Initialize the model.\n",
    "        self.model = Seq2SeqAutoencoder(\n",
    "            src_emb_dim=256,\n",
    "            trg_emb_dim=256,\n",
    "            src_vocab_size=self.vocab_size,\n",
    "            src_hidden_dim=512,\n",
    "            trg_hidden_dim=512,\n",
    "            batch_size=self.batch_size,\n",
    "            bidirectional=True,\n",
    "            pad_token_src=self.pad_token_ind,\n",
    "            nlayers=2,\n",
    "            nlayers_trg=1,\n",
    "            dropout=0.,\n",
    "        ).cuda()\n",
    "        \n",
    "        # Restore from checkpoint if provided.\n",
    "        if checkpoint:\n",
    "            self.restore_model(checkpoint)\n",
    "        \n",
    "        # Initialize the optimizer.\n",
    "        self.lr = 0.0002\n",
    "        self.clip_c = 1\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Loss criterion.\n",
    "        weight_mask = torch.ones(self.vocab_size).cuda()\n",
    "        weight_mask[self.pad_token_ind] = 0\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(weight=weight_mask).cuda()\n",
    "        \n",
    "        # Save the initial model.\n",
    "        self.save_model()\n",
    "        \n",
    "    def clip_gradient(self, model, clip):\n",
    "        \"\"\"Compute a gradient clipping coefficient based on gradient norm.\"\"\"\n",
    "        totalnorm = 0\n",
    "        for p in self.model.parameters():\n",
    "            modulenorm = p.grad.data.norm()\n",
    "            totalnorm += modulenorm ** 2\n",
    "        totalnorm = math.sqrt(totalnorm)\n",
    "        return min(1, clip / (totalnorm + 1e-6))\n",
    "    \n",
    "    def get_dataset_minibatch(self, examples, iter_ind, batch_size):\n",
    "        \"\"\"\n",
    "        Iterator over the dataset split and get autoencoder minibatches.\n",
    "        \"\"\"\n",
    "        minibatch = examples[iter_ind:iter_ind+batch_size]\n",
    "        \n",
    "        # Create the Pytorch variables.\n",
    "        input_lines = Variable(torch.LongTensor(np.fliplr(minibatch).copy())).cuda() # Reverse the input lines.\n",
    "        output_lines = Variable(torch.LongTensor(minibatch)).cuda()\n",
    "        return input_lines, output_lines\n",
    "    \n",
    "    def perplexity(self):\n",
    "        \"\"\"Calculate the BLEU score.\"\"\"\n",
    "        \n",
    "    def train(self, dataset, epochs=2, write_checkpoint=1, monitor_loss=1, print_samples=1):\n",
    "        examples, _ = dataset.examples(split=\"train\", shuffled=True)\n",
    "        num_examples, max_len = examples.shape\n",
    "        \n",
    "        for epoch in xrange(epochs):\n",
    "            losses = []\n",
    "            for iter_ind in xrange(0, num_examples, self.batch_size):\n",
    "                # Get a minibatch.\n",
    "                input_lines_src, output_lines_src = self.get_dataset_minibatch(examples, iter_ind, self.batch_size)\n",
    "                \n",
    "                # Run a training step.\n",
    "                decoder_logit = self.model(input_lines_src)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                loss = self.loss_criterion(\n",
    "                    decoder_logit.contiguous().view(-1, self.vocab_size),\n",
    "                    output_lines_src.view(-1)\n",
    "                )\n",
    "                losses.append(loss.data[0])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if iter_ind % monitor_loss == 0:\n",
    "                    # TODO(cathywong): change to logging.\n",
    "                    print('Epoch : %d Minibatch : %d Loss : %.5f' % (epoch, iter_ind, np.mean(losses)))\n",
    "                    losses = []\n",
    "                \n",
    "                if iter_ind % print_samples == 0:\n",
    "                    # Print samples.\n",
    "                    word_probs = self.model.decode(decoder_logit).data.cpu().numpy().argmax(axis=-1)\n",
    "                    output_lines_trg = input_lines_src.data.cpu().numpy()\n",
    "                    for sentence_pred, sentence_real in zip(word_probs[:5], output_lines_trg[:5]):\n",
    "                        decoded_real = dataset.vocab_encoder.decode_numpy(sentence_real[::-1])\n",
    "                        decoded_pred = dataset.vocab_encoder.decode_numpy(sentence_pred)\n",
    "                        \n",
    "                        # TODO(cathywong): change to logging.\n",
    "                        print decoded_pred\n",
    "                        print decoded_real\n",
    "                break\n",
    "            # Write checkpoint.\n",
    "            if epoch % write_checkpoint == 0:\n",
    "                self.save_model()\n",
    "            \n",
    "        \n",
    "    def evaluate(self, dataset, split, verbose=True):\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def save_model(self, \n",
    "                   checkpoint_dir='/cvgl2/u/catwong/cs332_final_project/checkpoints',\n",
    "                   checkpoint_name='seq2seq_autoencoder'):\n",
    "        # Outputs a path that can be passed into the restore.\n",
    "        checkpoint_file = checkpoint_name + '.model'\n",
    "        full_checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "        torch.save(\n",
    "            self.model.state_dict(),\n",
    "            open(full_checkpoint_path, 'wb')\n",
    "        )\n",
    "        return full_checkpoint_path\n",
    "    \n",
    "    def restore_model(self, checkpoint):\n",
    "        self.model.load_state_dict(torch.load(open(checkpoint)))\n",
    "\n",
    "# Demo\n",
    "spam_dataset = SpamDataset()\n",
    "autoencoder = SpamSeq2SeqAutoencoder(dataset=spam_dataset)\n",
    "autoencoder.train(spam_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Text Example Experiment Runner Template\n",
    "11/6/2017 - Basic pipeline to run adversarial text generation experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "Base dataset: The Enron Spam Dataset: http://www2.aueb.gr/users/ion/data/enron-spam/ \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Spam Preprocessing - UNIX Command line\n",
    "# 1. Removed all \\n and replaced with spaces: find . -type f -exec perl -i. -pe 's/\\r?\\n/ /' {} +\n",
    "# 2. Concatenated all spam into a single file and all ham into a single file.\n",
    "#      To concatenate within dirs: awk 1 enron1/ham/*.txt > enron1_ham.txt \n",
    "# 3. Randomly shuffled: shuf input > output\n",
    "# 4. Create 80, 10, 10 train, val, and test splits.\n",
    "\n",
    "# Total ham: 16545 messages; train/val/test = 13236, 1654, 1655\n",
    "# Total spam: 17171 messages; train/val/test = 13736, 1717, 1718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/spam.txt\n",
      "Now processing: /cvgl2/u/catwong/cs332_final_project/data/train/ham.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "classes = ['spam', 'ham']\n",
    "vocabulary_filename = 'email_train_vocab.txt' \n",
    "# Truncation and vocabulary shortening:\n",
    "# Using the train data only:\n",
    "# 1. Truncate both the spam and ham messages to truncation_len characters (adding padding where needed).\n",
    "# 2. From the truncated messages, compile a vocabulary of the class_vocabulary_size most frequent tokens for each class.\n",
    "# 3. Write a vocabulary file composed of the full, combined vocabulary (ie. the most frequent tokens across both classes.)\n",
    "truncation_len = 100\n",
    "class_vocabulary_size = 3000\n",
    "\n",
    "combined_vocab = []\n",
    "for class_name in classes:\n",
    "    filename = os.path.join(base_data_dir, 'train', class_name + '.txt') \n",
    "    print \"Now processing: %s\" % filename\n",
    "    with open(filename) as f:\n",
    "        all_lines = [line.strip().lower().split() for line in f.readlines()]\n",
    "        \n",
    "    # Truncate the files.\n",
    "    truncated_lines = [line[:truncation_len] for line in all_lines]\n",
    "    \n",
    "    # Add tokens to the counter\n",
    "    token_counts = Counter()\n",
    "    for line in truncated_lines:\n",
    "        token_counts.update(line)\n",
    "    combined_vocab += [elem for (elem, count) in token_counts.most_common(class_vocabulary_size)]\n",
    "    # Convert the combined vocabulary into a set.\n",
    "combined_vocab = set(combined_vocab)\n",
    "# Write out the combined_vocab to the vocabulary file\n",
    "with open(vocabulary_filename, 'w') as f:\n",
    "    for token in combined_vocab:\n",
    "        f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: does your business depend on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple search engines . submit your website online and watch visitors stream to your e - business . best regards , myrtice melendez\n",
      "<SOS> subject: does your business <UNK> on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple <EOS>\n",
      "0 542 4302 2161 3459 2 2124 838 1689 2699 2129 2161 731 4305 1068 2161 731 781 2427 714 1398 1543 2161 1689 2227 3407 720 788 2256 1116 1136 1658 2001 3244 2161 731 302 2256 2973 183 1593 2161 731 1689 4060 772 4125 3203 2947 3062 302 3879 3960 997 3094 781 699 720 788 2256 217 925 2336 4252 4323 2161 731 1658 1246 2161 205 302 838 1756 1032 2336 585 1625 771 2336 793 2161 4140 137 781 4395 1789 925 2427 949 1397 302 4193 720 3351 720 1593 2161 731 781 1696 1\n",
      "<SOS> subject: does your business <UNK> on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Class that takes in a file and a vocabulary file (which has a truncation len) and converts the text into\n",
    "# encoded/truncated sentences.\n",
    "\n",
    "class DatasetEncoderDecoder(object):\n",
    "    \"\"\"\n",
    "    Encodes and decodes sentences according to a vocabulary.\n",
    "    \n",
    "    Sentences are truncated. OOV words are assigned an <UNK> token, and <SOS>, <PAD>, and <EOS> tokens are added.\n",
    "    \n",
    "    truncation_len\n",
    "    \"\"\"\n",
    "    def __init__(self, truncation_len, vocab_file):\n",
    "        self.truncation_len = truncation_len\n",
    "        # Create index to word and word to index dicts from the vocab_file.\n",
    "        num_default_tokens = 4\n",
    "        self.index2word = {0:'<SOS>', 1:'<EOS>', 2: '<UNK>', 3: '<PAD>'}\n",
    "        self.word2index = {'<SOS>':0, '<EOS>':1, '<UNK>': 2, '<PAD>': 3}\n",
    "        with open(vocab_file) as f:\n",
    "            all_lines = [line.strip() for line in f.readlines()]\n",
    "        for idx, token in enumerate(all_lines):\n",
    "            self.index2word[idx + num_default_tokens] = token\n",
    "            self.word2index[token] = idx + num_default_tokens\n",
    "          \n",
    "    def encode_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Encodes a sentence according to the vocabulary.\n",
    "        Returns:\n",
    "            normalized: the normalized sentence, as it would be decoded.\n",
    "            encoded: the space-separated numerical sentence.\n",
    "        \"\"\"\n",
    "        truncated = sentence.lower().split()[:truncation_len]\n",
    "        truncated += ['<PAD>'] * max(truncation_len - len(truncated), 0)\n",
    "        truncated = ['<SOS>'] + truncated + ['<EOS>']\n",
    "        \n",
    "        normalized = []\n",
    "        encoded = []\n",
    "        # Encode, removing the UNK tokens\n",
    "        for token in truncated:\n",
    "            token = token if token in self.word2index else '<UNK>'\n",
    "            normalized.append(token)\n",
    "            encoded.append(str(self.word2index[token]))\n",
    "        \n",
    "        normalized = \" \".join(normalized)\n",
    "        encoded = \" \".join(encoded)\n",
    "        return normalized, encoded\n",
    "    \n",
    "    def decode_sentence(self, encoded):\n",
    "        \"\"\"Returns the decoded sentence.\"\"\"\n",
    "        numerical_encoded = [int(token) for token in encoded.split()]\n",
    "        return \" \".join([self.index2word[token] for token in numerical_encoded])\n",
    "\n",
    "# Demonstration:\n",
    "truncation_len = 100\n",
    "vocab_file = 'data/email_train_vocab.txt'\n",
    "sample_text = 'Subject: does your business depend on the online success of your website ? submitting your website in search engines may increase your online sales dramatically . if you invested time and money into your website , you simply must submit your website online otherwise it will be invisible virtually , which means efforts spent in vain . if you want people to know about your website and boost your revenues , the only way to do that is to make your site visible in places where people search for information , i . e . submit your website in multiple search engines . submit your website online and watch visitors stream to your e - business . best regards , myrtice melendez'\n",
    "demo = DatasetEncoderDecoder(truncation_len, vocab_file)\n",
    "normalized, encoded = demo.encode_sentence(sample_text)\n",
    "print sample_text\n",
    "print normalized\n",
    "print encoded\n",
    "decoded = demo.decode_sentence(encoded)\n",
    "print decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the train, test, and text encoded files using this encoder.\n",
    "base_data_dir = \"/cvgl2/u/catwong/cs332_final_project/data/\"\n",
    "splits = ['train', 'val', 'test']\n",
    "classes = ['spam.txt', 'ham.txt']\n",
    "truncation_len = 100\n",
    "vocab_file = 'data/email_train_vocab.txt'\n",
    "\n",
    "vocab_encoder = DatasetEncoderDecoder(truncation_len, vocab_file)\n",
    "for split in splits:\n",
    "    for class_file in classes:\n",
    "        raw_file = os.path.join(base_data_dir, split, class_file)\n",
    "        with open(raw_file) as f:\n",
    "            all_lines = [line.strip() for line in f.readlines()]\n",
    "        # Encode the lines\n",
    "        encoded_lines = [vocab_encoder.encode_sentence(line)[1] for line in all_lines]\n",
    "        \n",
    "        # Write out the encoded line\n",
    "        encoded_file = os.path.join(base_data_dir, split, 'encoded_' + class_file)\n",
    "        with open(encoded_file, 'w') as f:\n",
    "            for line in encoded_lines:\n",
    "                f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder - Seq2Seq Model\n",
    "Source: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
